{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDlxBSY/lXclQrxTQhMOqg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nor8SVCw0EK-","executionInfo":{"status":"ok","timestamp":1725449736555,"user_tz":-330,"elapsed":54239,"user":{"displayName":"Sai Ganesh","userId":"09373529376326207367"}},"outputId":"79a665d1-b3eb-4a0f-d281-a5afdfdaf2a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=f3e330dc3d3365f081b5373fda1af2065d258fce9150ab1253aa248747101123\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}],"source":["pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","\n","# Initialize a Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Advanced DataFrame Operations - Different Dataset\") \\\n","    .getOrCreate()\n","\n","# Create two sample DataFrames for Product Sales\n","data1 = [\n","    (1, 'Product A', 'Electronics', 1200, '2022-05-10'),\n","    (2, 'Product B', 'Clothing', 500, '2022-07-15'),\n","    (3, 'Product C', 'Electronics', 1800, '2021-11-05')\n","]\n","\n","data2 = [\n","    (4, 'Product D', 'Furniture', 3000, '2022-03-25'),\n","    (5, 'Product E', 'Clothing', 800, '2022-09-12'),\n","    (6, 'Product F', 'Electronics', 1500, '2021-10-19')\n","]\n","\n","# Define schema (columns)\n","columns = ['ProductID', 'ProductName', 'Category', 'Price', 'SaleDate']\n","\n","# Create DataFrames\n","sales_df1 = spark.createDataFrame(data1, columns)\n","sales_df2 = spark.createDataFrame(data2, columns)\n","\n","# show dataframes\n","sales_df1.show()\n","sales_df2.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFAVwVFK0IAx","executionInfo":{"status":"ok","timestamp":1725449783203,"user_tz":-330,"elapsed":21523,"user":{"displayName":"Sai Ganesh","userId":"09373529376326207367"}},"outputId":"ebf6c314-2ffb-42ef-c6d0-35b96bb4e118"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-----------+-----------+-----+----------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|\n","+---------+-----------+-----------+-----+----------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|\n","|        2|  Product B|   Clothing|  500|2022-07-15|\n","|        3|  Product C|Electronics| 1800|2021-11-05|\n","+---------+-----------+-----------+-----+----------+\n","\n","+---------+-----------+-----------+-----+----------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|\n","+---------+-----------+-----------+-----+----------+\n","|        4|  Product D|  Furniture| 3000|2022-03-25|\n","|        5|  Product E|   Clothing|  800|2022-09-12|\n","|        6|  Product F|Electronics| 1500|2021-10-19|\n","+---------+-----------+-----------+-----+----------+\n","\n"]}]},{"cell_type":"code","source":["# Tasks\n","\n","#  1. Union of dataframes(Removing duplicates)\n","# Combine the two DataFrames (`sales_df1` and `sales_df2`) using `union` and remove any duplicate rows\n","combined_df = sales_df1.union(sales_df2).dropDuplicates()\n","print(\"Union of dataframes (removing duplicates)\")\n","combined_df.show()\n","\n","# 2. Union of DataFrames (including duplicates)**:\n","# Combine both DataFrames using `unionAll` (replaced by `union`) and include duplicate rows\n","combined_all_df = sales_df1.unionAll(sales_df2)\n","print(\"Union of dataframes (including duplicates): \")\n","combined_all_df.show()\n","\n","# 3. Rank products by price within their category:\n","# Use window functions to rank the products in each category by price in descending order\n","window_spec = Window.partitionBy('Category').orderBy(F.desc('Price'))\n","ranked_df = combined_df.withColumn('Rank', F.row_number().over(window_spec))\n","print(\"Rank of the products in descending order\")\n","ranked_df.show()\n","\n","# 4. Calculate cumulative price per category\n","# Use window functions to calculate the cumulative price of products within each category\n","sales_df_cumulative = combined_df.withColumn('CumulativePrice', F.sum('Price').over(window_spec))\n","print(\"Cumulative price of products per each category: \")\n","sales_df_cumulative.show()\n","\n","# 5. Convert SaleDate from string to date type\n","# Convert the SaleDate column from string format to a PySpark date type\n","sales_df_date_converted = combined_df.withColumn('SaleDate', F.to_date('SaleDate', 'yyyy-MM-dd'))\n","print(\"Saledate from string to date type: \")\n","sales_df_date_converted.show()\n","\n","# 6. Calculate the number of days since each sale\n","# Calculate the number of days since each product was sold using the current date\n","sales_df_days_since = combined_df.withColumn('DaysSinceSale', F.datediff(F.current_date(), 'SaleDate'))\n","print(\"No.of days since each product sold: \")\n","sales_df_days_since.show()\n","\n","# 7. Add a column for the next sale deadline\n","# Add a new column NextSaleDeadline, which should be 30 days after the SaleDate\n","sales_df_next_deadline = combined_df.withColumn('NextSaleDeadline', F.date_add('SaleDate', 30))\n","print(\"Add a column for the next sale deadline: \")\n","sales_df_next_deadline.show()\n","\n","# 8.Calculate total revenue and average price per category\n","# Find the total revenue (sum of prices) and the average price per category\n","total_revenue_df = combined_df.groupBy('Category').agg(\n","    F.sum('Price').alias('TotalRevenue'),\n","    F.avg('Price').alias('AveragePrice')\n",")\n","print(\"Total revenue and average price per category: \")\n","total_revenue_df.show()\n","\n","# 9. Convert all product names to lowercase\n","# Create a new column with all product names in lowercase\n","sales_df_lowercase = combined_df.withColumn('ProductNameLower', F.lower('ProductName'))\n","print(\"Convert all product names to lowercase: \")\n","sales_df_lowercase.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSTcRsNU0m60","executionInfo":{"status":"ok","timestamp":1725450823543,"user_tz":-330,"elapsed":11230,"user":{"displayName":"Sai Ganesh","userId":"09373529376326207367"}},"outputId":"f38076b5-bafc-4cbc-fab9-f4926c2aadd8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Union of dataframes (removing duplicates)\n","+---------+-----------+-----------+-----+----------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|\n","+---------+-----------+-----------+-----+----------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|\n","|        2|  Product B|   Clothing|  500|2022-07-15|\n","|        3|  Product C|Electronics| 1800|2021-11-05|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|\n","|        6|  Product F|Electronics| 1500|2021-10-19|\n","|        5|  Product E|   Clothing|  800|2022-09-12|\n","+---------+-----------+-----------+-----+----------+\n","\n","Union of dataframes (including duplicates): \n","+---------+-----------+-----------+-----+----------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|\n","+---------+-----------+-----------+-----+----------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|\n","|        2|  Product B|   Clothing|  500|2022-07-15|\n","|        3|  Product C|Electronics| 1800|2021-11-05|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|\n","|        5|  Product E|   Clothing|  800|2022-09-12|\n","|        6|  Product F|Electronics| 1500|2021-10-19|\n","+---------+-----------+-----------+-----+----------+\n","\n","Rank of the products in descending order\n","+---------+-----------+-----------+-----+----------+----+\n","|ProductID|ProductName|   Category|Price|  SaleDate|Rank|\n","+---------+-----------+-----------+-----+----------+----+\n","|        5|  Product E|   Clothing|  800|2022-09-12|   1|\n","|        2|  Product B|   Clothing|  500|2022-07-15|   2|\n","|        3|  Product C|Electronics| 1800|2021-11-05|   1|\n","|        6|  Product F|Electronics| 1500|2021-10-19|   2|\n","|        1|  Product A|Electronics| 1200|2022-05-10|   3|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|   1|\n","+---------+-----------+-----------+-----+----------+----+\n","\n","Cumulative price of products per each category: \n","+---------+-----------+-----------+-----+----------+---------------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|CumulativePrice|\n","+---------+-----------+-----------+-----+----------+---------------+\n","|        5|  Product E|   Clothing|  800|2022-09-12|            800|\n","|        2|  Product B|   Clothing|  500|2022-07-15|           1300|\n","|        3|  Product C|Electronics| 1800|2021-11-05|           1800|\n","|        6|  Product F|Electronics| 1500|2021-10-19|           3300|\n","|        1|  Product A|Electronics| 1200|2022-05-10|           4500|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|           3000|\n","+---------+-----------+-----------+-----+----------+---------------+\n","\n","Saledate from string to date type: \n","+---------+-----------+-----------+-----+----------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|\n","+---------+-----------+-----------+-----+----------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|\n","|        2|  Product B|   Clothing|  500|2022-07-15|\n","|        3|  Product C|Electronics| 1800|2021-11-05|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|\n","|        6|  Product F|Electronics| 1500|2021-10-19|\n","|        5|  Product E|   Clothing|  800|2022-09-12|\n","+---------+-----------+-----------+-----+----------+\n","\n","No.of days since each product sold: \n","+---------+-----------+-----------+-----+----------+-------------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|DaysSinceSale|\n","+---------+-----------+-----------+-----+----------+-------------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|          848|\n","|        2|  Product B|   Clothing|  500|2022-07-15|          782|\n","|        3|  Product C|Electronics| 1800|2021-11-05|         1034|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|          894|\n","|        6|  Product F|Electronics| 1500|2021-10-19|         1051|\n","|        5|  Product E|   Clothing|  800|2022-09-12|          723|\n","+---------+-----------+-----------+-----+----------+-------------+\n","\n","Add a column for the next sale deadline: \n","+---------+-----------+-----------+-----+----------+----------------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|NextSaleDeadline|\n","+---------+-----------+-----------+-----+----------+----------------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|      2022-06-09|\n","|        2|  Product B|   Clothing|  500|2022-07-15|      2022-08-14|\n","|        3|  Product C|Electronics| 1800|2021-11-05|      2021-12-05|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|      2022-04-24|\n","|        6|  Product F|Electronics| 1500|2021-10-19|      2021-11-18|\n","|        5|  Product E|   Clothing|  800|2022-09-12|      2022-10-12|\n","+---------+-----------+-----------+-----+----------+----------------+\n","\n","Total revenue and average price per category: \n","+-----------+------------+------------+\n","|   Category|TotalRevenue|AveragePrice|\n","+-----------+------------+------------+\n","|Electronics|        4500|      1500.0|\n","|   Clothing|        1300|       650.0|\n","|  Furniture|        3000|      3000.0|\n","+-----------+------------+------------+\n","\n","Convert all product names to lowercase: \n","+---------+-----------+-----------+-----+----------+----------------+\n","|ProductID|ProductName|   Category|Price|  SaleDate|ProductNameLower|\n","+---------+-----------+-----------+-----+----------+----------------+\n","|        1|  Product A|Electronics| 1200|2022-05-10|       product a|\n","|        2|  Product B|   Clothing|  500|2022-07-15|       product b|\n","|        3|  Product C|Electronics| 1800|2021-11-05|       product c|\n","|        4|  Product D|  Furniture| 3000|2022-03-25|       product d|\n","|        6|  Product F|Electronics| 1500|2021-10-19|       product f|\n","|        5|  Product E|   Clothing|  800|2022-09-12|       product e|\n","+---------+-----------+-----------+-----+----------+----------------+\n","\n"]}]}]}