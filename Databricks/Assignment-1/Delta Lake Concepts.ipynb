{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab418f0-6274-4069-9add-4a24d105cb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/employees.csv\",\"dbfs:/FileStore/employees.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9aef05b-da58-4617-b544-55e4ff6d1e90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\",\"dbfs:/FileStore/products.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7e4a03-1657-426b-bf98-10004dbfe57f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       102|       Alice|    Finance| 2023-02-15| 70000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the given CSV and JSON datasets into Databricks.\n",
    "# Load the CSV file into dataframe\n",
    "employees_df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"dbfs:/FileStore/employees.csv\")\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3684f0-37fa-48ae-acf7-afe7bf4c9201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the JSON file into dataframe\n",
    "products_df=spark.read.format(\"json\").load(\"dbfs:/FileStore/products.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbb4212-c523-4338-a7e0-97e823853a72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       102|       Alice|    Finance| 2023-02-15| 70000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a Delta Table using the following three methods\n",
    "# Save DataFrame as Delta Table\n",
    "employees_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employees\")\n",
    "# Load Delta Table\n",
    "delta_employees = spark.read.format(\"delta\").load(\"/delta/employees\")\n",
    "# Show Delta Table\n",
    "delta_employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b80c7db-abba-4643-828d-8eb7bcdef154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save products DataFrame as Delta Table\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/products\")\n",
    "delta_products = spark.read.format(\"delta\").load(\"/delta/products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae1a39a-e642-49d2-9472-80da40097230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- 2. Use SQL to Create a Delta Table\n",
    "-- Create Delta Table from SQL for employees\n",
    "CREATE TABLE IF NOT EXISTS delta.`/delta/employees_sql`\n",
    "AS SELECT * FROM csv.`dbfs:/FileStore/employees.csv`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e71b669a-ab85-49dc-907f-70106710720b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create Delta Table from SQL for products\n",
    "CREATE OR REPLACE TABLE delta.`/delta/products_sql`\n",
    "AS SELECT * FROM json.`dbfs:/FileStore/products.json`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d937ce-3f6e-427c-b3a0-bb362035a832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees csv converted to Delta format\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV to Delta (Employees)\n",
    "employees_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/employees.csv\")\n",
    "employees_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employees_converted\")\n",
    "print(\"Employees csv converted to Delta format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fb041c9-61e1-4ee1-9ebf-d1c091d0d3f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_df = spark.read.format(\"json\").load(\"dbfs:/FileStore/products.json\")\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/products_converted\")\n",
    "print(\"Products json converted to Delta format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53fbbf4b-a3e6-482d-b456-b12a06257ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       102|       Alice|    Finance| 2023-02-15| 70000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Delta Table for Employees Created in Task 1\n",
    "employees_delta = spark.read.format(\"delta\").load(\"/delta/employees\")\n",
    "employees_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ef5bce-a73f-4df1-8cae-20e4e128717c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----------+------+\n|EmployeeID|EmployeeName|Department|JoiningDate|Salary|\n+----------+------------+----------+-----------+------+\n|       102|       Alice|   Finance| 2023-02-15| 75000|\n|       106|      Olivia|        HR| 2023-06-10| 65000|\n+----------+------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Creating a DataFrame for the New Employee Data\n",
    "new_data = [(102, \"Alice\", \"Finance\", \"2023-02-15\", 75000), \n",
    "            (106, \"Olivia\", \"HR\", \"2023-06-10\", 65000)] \n",
    "\n",
    "columns = [\"EmployeeID\", \"EmployeeName\", \"Department\", \"JoiningDate\", \"Salary\"]\n",
    "\n",
    "new_employees_df = spark.createDataFrame(new_data, columns)\n",
    "new_employees_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d95a2b8-6dbd-4e12-8243-2022ad22b883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n|       102|       Alice|    Finance| 2023-02-15| 75000|\n|       106|      Olivia|         HR| 2023-06-10| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Merge the New Employee Data into the Existing Employees Delta Table\n",
    "from delta.tables import *\n",
    "\n",
    "employees_delta_table = DeltaTable.forPath(spark, \"/delta/employees\")\n",
    "\n",
    "employees_delta_table.alias(\"employees\").merge(\n",
    "    new_employees_df.alias(\"new_data\"),\n",
    "    \"employees.EmployeeID = new_data.EmployeeID\"  \n",
    ").whenMatchedUpdate(set = {\n",
    "    \"employees.Salary\": \"new_data.Salary\" \n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"EmployeeID\": \"new_data.EmployeeID\",  \n",
    "    \"EmployeeName\": \"new_data.EmployeeName\",\n",
    "    \"Department\": \"new_data.Department\",\n",
    "    \"JoiningDate\": \"new_data.JoiningDate\",\n",
    "    \"Salary\": \"new_data.Salary\"\n",
    "}).execute()\n",
    "\n",
    "updated_employees_df = spark.read.format(\"delta\").load(\"/delta/employees\")\n",
    "updated_employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a35746-0a0c-4e6b-9fb1-f41f14fbc5df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+---------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------+---------------------------------------------------------------+\n|format|id                                  |name|description|location             |createdAt              |lastModified       |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                           |minReaderVersion|minWriterVersion|tableFeatures    |statistics                                                     |\n+------+------------------------------------+----+-----------+---------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------+---------------------------------------------------------------+\n|delta |5b3d183d-aac7-4289-b13f-212437bebae5|NULL|NULL       |dbfs:/delta/employees|2024-09-17 08:08:29.826|2024-09-17 08:17:50|[]              |[]               |1       |1428       |{delta.enableDeletionVectors -> true}|3               |7               |[deletionVectors]|{numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0}|\n+------+------------------------------------+----+-----------+---------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------+---------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Explore the internals of the employees Delta table using Delta Lake features.\n",
    "employees_delta_table = DeltaTable.forPath(spark, \"/delta/employees\")\n",
    "employees_delta_table.detail().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d951cf-973d-41b2-a546-33d7de88a255",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                                                 |job |notebook         |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|2      |2024-09-17 08:17:50|8838676295264951|azuser2125_mml.local@techademy.com|OPTIMIZE |{predicate -> [], zOrderBy -> [], batchId -> 0, auto -> true}                                                                                                                                                                       |NULL|{652365119865072}|0911-091631-qlq2ghxz|1          |SnapshotIsolation|false        |{numRemovedFiles -> 3, numRemovedBytes -> 3756, p25FileSize -> 1428, numDeletionVectorsRemoved -> 1, minFileSize -> 1428, numAddedFiles -> 1, maxFileSize -> 1428, p75FileSize -> 1428, p50FileSize -> 1428, numAddedBytes -> 1428}                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n|1      |2024-09-17 08:17:46|8838676295264951|azuser2125_mml.local@techademy.com|MERGE    |{predicate -> [\"(cast(EmployeeID#1954 as bigint) = EmployeeID#1913L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{652365119865072}|0911-091631-qlq2ghxz|0          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 2384, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 4930, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2582, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1966}|NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n|0      |2024-09-17 08:08:33|8838676295264951|azuser2125_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                                        |NULL|{652365119865072}|0911-091631-qlq2ghxz|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1372}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check the transaction history of the Delta table\n",
    "history_df = employees_delta_table.history()\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e485417-6684-4f51-914a-0b10113afcb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n|       102|       Alice|    Finance| 2023-02-15| 75000|\n|       106|      Olivia|         HR| 2023-06-10| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform Time Travel and retrieve the table before the previous merge operation.\n",
    "previous_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/employees\")\n",
    "previous_version_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661cbf72-5399-4455-a5d2-f9c67b6fb887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize the Delta table for better performance\n",
    "spark.sql(\"OPTIMIZE delta.`/delta/employees`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a54177-1883-47e8-b31c-ca2cb51eb7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optimize the Delta table and apply Z-ordering on the Department column\n",
    "spark.sql(\"OPTIMIZE delta.`/delta/employees` ZORDER BY (Department)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1157709-285a-4eb1-9986-7683cd69a083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                                                 |job |notebook         |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|2      |2024-09-17 08:17:50|8838676295264951|azuser2125_mml.local@techademy.com|OPTIMIZE |{predicate -> [], zOrderBy -> [], batchId -> 0, auto -> true}                                                                                                                                                                       |NULL|{652365119865072}|0911-091631-qlq2ghxz|1          |SnapshotIsolation|false        |{numRemovedFiles -> 3, numRemovedBytes -> 3756, p25FileSize -> 1428, numDeletionVectorsRemoved -> 1, minFileSize -> 1428, numAddedFiles -> 1, maxFileSize -> 1428, p75FileSize -> 1428, p50FileSize -> 1428, numAddedBytes -> 1428}                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n|1      |2024-09-17 08:17:46|8838676295264951|azuser2125_mml.local@techademy.com|MERGE    |{predicate -> [\"(cast(EmployeeID#1954 as bigint) = EmployeeID#1913L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{652365119865072}|0911-091631-qlq2ghxz|0          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 2384, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 4930, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2582, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1966}|NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n|0      |2024-09-17 08:08:33|8838676295264951|azuser2125_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                                        |NULL|{652365119865072}|0911-091631-qlq2ghxz|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1372}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check the transaction history of the Delta table\n",
    "employees_delta_table = DeltaTable.forPath(spark, \"/delta/employees\")\n",
    "history_df = employees_delta_table.history()\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f85ffcc-a3a5-4946-8ff2-2a647102f373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n|       102|       Alice|    Finance| 2023-02-15| 75000|\n|       106|      Olivia|         HR| 2023-06-10| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "previous_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/employees\")\n",
    "previous_version_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3b934a-12f2-4f1b-91fd-537521b90812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n|       102|       Alice|    Finance| 2023-02-15| 75000|\n|       106|      Olivia|         HR| 2023-06-10| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Query the Delta table at version 1\n",
    "specific_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/employees\")\n",
    "specific_version_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc32edfd-f7bd-4fdc-ad40-9fdbcb596318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vacuum the employees Delta table to remove old files (older than default retention of 7 days)\n",
    "spark.sql(\"VACUUM delta.`/delta/employees`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852106cc-3a72-4bf2-9aa4-8d188a80ebfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vacuum the employees Delta table and set the retention period to 7 days\n",
    "spark.sql(\"VACUUM delta.`/delta/employees` RETAIN 168 HOURS\"); "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 652365119865109,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta Lake Concepts",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
