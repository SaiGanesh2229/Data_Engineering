{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5c703c-6dff-4008-8c0d-7e2d62066955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/transaction.csv\", \"dbfs:/mnt/streaming/csv_files/transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d10706a-2b7a-4438-8578-3e9f2571a9fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/Products.csv\", \"dbfs:/mnt/streaming/csv_files/Products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2bed309-be58-4c4e-9805-2c6589ef936e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- TransactionID: string (nullable = true)\n |-- TransactionDate: string (nullable = true)\n |-- ProductID: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#  Set Up a Structured Streaming Source to Read CSV Data Continuously\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"TransactionDate\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .csv(\"/mnt/streaming/csv_files/\")\n",
    ")\n",
    "\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140d3622-a865-4073-b6ee-53beb831f36b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.  Ensure that the streaming query reads the data continuously in append mode and displays the results in the console.\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b87184-0877-4fb0-a764-c1e9be34aa34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- TransactionID: string (nullable = true)\n |-- TransactionDate: string (nullable = true)\n |-- ProductID: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n |-- TotalAmount: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Add a new column for the TotalAmount ( Quantity * Price )\n",
    "# Filter records where the Quantity is greater than 1\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "transformed_streaming_df = (\n",
    "    streaming_df\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
    "    .filter(col(\"Quantity\") > 1)\n",
    ")\n",
    "\n",
    "transformed_streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7dfa16-f9a0-46c0-8815-fed09dcdb03c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the transformed stream to a memory sink\n",
    "query = (\n",
    "    transformed_streaming_df.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"transformed_transactions\")  # The name of the in-memory table\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d901c6-2b23-4d24-ae0d-b093534c88c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------+--------+-----+-----------+\n|TransactionID|TransactionDate|ProductID|Quantity|Price|TotalAmount|\n+-------------+---------------+---------+--------+-----+-----------+\n|         T101|     2024-01-01|   Laptop|       2| 1200|       2400|\n|         T103|     2024-01-03|   Tablet|       3|  600|       1800|\n|         T105|     2024-01-05|    Mouse|       5|   25|        125|\n+-------------+---------------+---------+--------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM transformed_transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb4609c-fb35-4a66-bbc5-48ae8ec81751",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ProductID: string (nullable = true)\n |-- TotalSales: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#  Implement an aggregation on the streaming data\n",
    "#Group the data by ProductID and calculate the total sales for each product\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "aggregated_streaming_df = (\n",
    "    streaming_df\n",
    "    .groupBy(\"ProductID\")\n",
    "    .agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"TotalSales\"))\n",
    ")\n",
    "\n",
    "aggregated_streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f207c9d-9ee5-4071-9eed-dc4ca8e40d4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the stream runs in update mode, so only updated results are output to the sink.\n",
    "query = (\n",
    "    aggregated_streaming_df.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"update\")\n",
    "    .queryName(\"aggregated_sales\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7687d5d-f75f-4ea1-a792-fb4de8211e34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|ProductID|TotalSales|\n+---------+----------+\n|    Phone|       800|\n|   Laptop|      2400|\n|    Mouse|       125|\n|   Tablet|      1800|\n|  Monitor|       300|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM aggregated_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2911048-929d-4097-bf91-b8bfaa57dda5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After transforming and aggregating the data, write the streaming results to a Parquet sink.\n",
    "output_path = \"/mnt/streaming/output/parquet_files\"\n",
    "\n",
    "# Convert TransactionDate to TIMESTAMP type\n",
    "aggregated_streaming_df = transformed_streaming_df.withColumn(\n",
    "    \"TransactionDate\",\n",
    "    transformed_streaming_df[\"TransactionDate\"].cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Add a watermark to handle late data\n",
    "aggregated_streaming_df_with_watermark = aggregated_streaming_df.withWatermark(\n",
    "    \"TransactionDate\",\n",
    "    \"10 minutes\"\n",
    ")\n",
    "\n",
    "query = (\n",
    "    aggregated_streaming_df_with_watermark.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"path\", output_path)\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/output/checkpoints\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c58a41c-dcc0-4693-8f82-f007ca1c121c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Introduce a watermark on the TransactionDate column to handle late data arriving in the stream.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "watermarked_streaming_df = streaming_df.withColumn(\n",
    "    \"TransactionDate\",\n",
    "   streaming_df[\"TransactionDate\"].cast(\"timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451e4e7d-69df-4458-b696-7450a582f363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate Two Streams of Data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"TransactionDate\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4a13aa-a819-4bc9-b4ab-3ac74615cda0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/mnt/streaming/transaction_data\")\n",
    "\n",
    "# Ingest the streaming data\n",
    "transaction_stream = (\n",
    "    spark.readStream\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(transaction_schema)\n",
    "    .csv(\"/mnt/streaming/transaction_data\")\n",
    ")\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True)\n",
    "])\n",
    "\n",
    "product_df=(\n",
    "    spark.readStream\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(product_schema)\n",
    "    .csv(\"/mnt/streaming/csv_files/\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc9cc3b-d672-4a19-969b-3231b46b418a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ProductID: string (nullable = true)\n |-- TransactionID: string (nullable = true)\n |-- TransactionDate: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: double (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Category: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Perform a Join on the Two Streams\n",
    "# Perform an inner join on the ProductID column\n",
    "joined_stream = (\n",
    "    transaction_stream.alias(\"t\")\n",
    "    .join(product_df.alias(\"p\"), \"ProductID\", \"inner\")\n",
    ")\n",
    "\n",
    "joined_stream.printSchema()\n",
    "\n",
    "joined_stream_result = joined_stream.select(\n",
    "    \"t.TransactionID\", \"t.TransactionDate\", \"t.ProductID\", \"p.ProductName\", \"p.Category\", \"t.Quantity\", \"t.Price\"\n",
    ")\n",
    "\n",
    "query = (\n",
    "    joined_stream_result.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae48047-892d-41d3-a56a-a37e77b9a65e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------+--------+-----+-----------+\n|TransactionID|    TransactionDate|ProductID|Quantity|Price|TotalAmount|\n+-------------+-------------------+---------+--------+-----+-----------+\n|         T101|2024-01-01 00:00:00|   Laptop|       2| 1200|       2400|\n|         T103|2024-01-03 00:00:00|   Tablet|       3|  600|       1800|\n|         T105|2024-01-05 00:00:00|    Mouse|       5|   25|        125|\n+-------------+-------------------+---------+--------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Stop the streaming query\n",
    "query.stop()\n",
    "\n",
    "# Explore the results\n",
    "result_df = spark.read.parquet(\"/mnt/streaming/output/parquet_files\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a450f1a2-330d-4e9a-ba86-d0aa0849eb28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart the streaming query using the same checkpoint location\n",
    "query = (\n",
    "    joined_stream_result.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"path\", \"/mnt/streaming/output/parquet_with_watermark\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/output/checkpoints_with_watermark\")\n",
    "    .start()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming and Transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
